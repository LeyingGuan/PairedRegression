system.time(out4 <- PERMtest(x= dat$x, y = dat$y, z=dat$Z, B = B))
mean(out3$res$unsigned<=alpha )
mean(out4$res$unsigned<=alpha )
# system.time(out2 <- permutation_PREGSj
setwd("/Users/lg689/Dropbox/exactInference/code/PairedRegression")
library(PREGS)
source("simulations_helpers.R")
set.seed(2024)
B = 2000
M = 2000
args = list()
###uncorrelated gaussian design, should suffer less from
args$D = "AnovaBalance"; args$E = "multinomial"; args$S=0.0; args$p=20;args$n=100
dat = data_gen(n=as.integer(args$n), p = as.integer(args$p), M = M, design =args$D, noise = args$E,
seed = seeds[it], m = 1)
if(as.numeric(args$S) <=0.0){
dat$beta = 0.0
}else{
dat$beta  = beta_gen(dat, power = as.numeric(args$S))
}
dat$y = y_gen(dat$x, dat$beta, dat$epsMat)
###t test
fitted_ttest = summary(lm(dat$y~dat$x+dat$Z))
pval_ttest = as.vector(sapply(fitted_ttest, function(z) z$coefficients[2,4]))
mean(pval_ttest<=0.05)
# ###append randomized rows
# L =50
# pval_ttest_noisy = matrix(NA, M,ncol = L)
# for(l in 1:L){
#   fitted_ttest1 = summary(lm(dat$y~dat$Z+dat$Z[sample(1:args$n, args$n, replace=F),]+dat$x))
#   pval_ttest_noisy[,l] = as.vector(sapply(fitted_ttest1, function(z) z$coefficients[nrow(z$coefficients),4]))
# }
# apply(pval_ttest_noisy<=0.05, 2,mean)
###is this because of the randomness in the permutation that mess things up?
# system.time(out1 <- permutation_PREGSseparate(x= dat$x, Y = dat$y, Z=dat$Z, B = B))
# system.time(out3 <- permutation_FL( x = dat$x, Y = dat$y,  Z=dat$Z, B = B))
# system.time(out4 <-permutation_vanilla(x = dat$x, Y = dat$y,  Z=dat$Z, B = B))
#
system.time(out1 <- PREGtest(x= dat$x, y = dat$y, z=dat$Z, B = B, mode = "separate"))
system.time(out2 <- PREGtest(x= dat$x, y = dat$y, z=dat$Z, B = B, mode = "jont"))
alpha = 0.05
mean(out1$res$unsigned<=alpha )
mean(out2$res$unsigned<=alpha )
mean(pval_ttest<=alpha)
system.time(out3 <- FLtest(x= dat$x, y = dat$y, z=dat$Z, B = B))
system.time(out4 <- PERMtest(x= dat$x, y = dat$y, z=dat$Z, B = B))
mean(out3$res$unsigned<=alpha )
mean(out4$res$unsigned<=alpha )
set.seed(2024)
B = 2000
M = 2000
args = list()
###uncorrelated gaussian design, should suffer less from
args$D = "AnovaBalance"; args$E = "multinomial"; args$S=0.0; args$p=21;args$n=100
dat = data_gen(n=as.integer(args$n), p = as.integer(args$p), M = M, design =args$D, noise = args$E,
seed = seeds[it], m = 1)
if(as.numeric(args$S) <=0.0){
dat$beta = 0.0
}else{
dat$beta  = beta_gen(dat, power = as.numeric(args$S))
}
dat$y = y_gen(dat$x, dat$beta, dat$epsMat)
###t test
fitted_ttest = summary(lm(dat$y~dat$x+dat$Z))
pval_ttest = as.vector(sapply(fitted_ttest, function(z) z$coefficients[2,4]))
mean(pval_ttest<=0.05)
# ###append randomized rows
# L =50
# pval_ttest_noisy = matrix(NA, M,ncol = L)
# for(l in 1:L){
#   fitted_ttest1 = summary(lm(dat$y~dat$Z+dat$Z[sample(1:args$n, args$n, replace=F),]+dat$x))
#   pval_ttest_noisy[,l] = as.vector(sapply(fitted_ttest1, function(z) z$coefficients[nrow(z$coefficients),4]))
# }
# apply(pval_ttest_noisy<=0.05, 2,mean)
###is this because of the randomness in the permutation that mess things up?
# system.time(out1 <- permutation_PREGSseparate(x= dat$x, Y = dat$y, Z=dat$Z, B = B))
# system.time(out3 <- permutation_FL( x = dat$x, Y = dat$y,  Z=dat$Z, B = B))
# system.time(out4 <-permutation_vanilla(x = dat$x, Y = dat$y,  Z=dat$Z, B = B))
#
system.time(out1 <- PREGtest(x= dat$x, y = dat$y, z=dat$Z, B = B, mode = "separate"))
system.time(out2 <- PREGtest(x= dat$x, y = dat$y, z=dat$Z, B = B, mode = "jont"))
alpha = 0.05
mean(out1$res$unsigned<=alpha )
mean(out2$res$unsigned<=alpha )
mean(pval_ttest<=alpha)
system.time(out3 <- FLtest(x= dat$x, y = dat$y, z=dat$Z, B = B))
system.time(out4 <- PERMtest(x= dat$x, y = dat$y, z=dat$Z, B = B))
mean(out3$res$unsigned<=alpha )
mean(out4$res$unsigned<=alpha )
set.seed(2024)
B = 2000
M = 2000
args = list()
###uncorrelated gaussian design, should suffer less from
args$D = "Anova"; args$E = "multinomial"; args$S=0.0; args$p=6;args$n=100
dat = data_gen(n=as.integer(args$n), p = as.integer(args$p), M = M, design =args$D, noise = args$E,
seed = seeds[it], m = 1)
if(as.numeric(args$S) <=0.0){
dat$beta = 0.0
}else{
dat$beta  = beta_gen(dat, power = as.numeric(args$S))
}
dat$y = y_gen(dat$x, dat$beta, dat$epsMat)
###t test
fitted_ttest = summary(lm(dat$y~dat$x+dat$Z))
pval_ttest = as.vector(sapply(fitted_ttest, function(z) z$coefficients[2,4]))
mean(pval_ttest<=0.05)
# ###append randomized rows
# L =50
# pval_ttest_noisy = matrix(NA, M,ncol = L)
# for(l in 1:L){
#   fitted_ttest1 = summary(lm(dat$y~dat$Z+dat$Z[sample(1:args$n, args$n, replace=F),]+dat$x))
#   pval_ttest_noisy[,l] = as.vector(sapply(fitted_ttest1, function(z) z$coefficients[nrow(z$coefficients),4]))
# }
# apply(pval_ttest_noisy<=0.05, 2,mean)
###is this because of the randomness in the permutation that mess things up?
# system.time(out1 <- permutation_PREGSseparate(x= dat$x, Y = dat$y, Z=dat$Z, B = B))
# system.time(out3 <- permutation_FL( x = dat$x, Y = dat$y,  Z=dat$Z, B = B))
# system.time(out4 <-permutation_vanilla(x = dat$x, Y = dat$y,  Z=dat$Z, B = B))
#
system.time(out1 <- PREGtest(x= dat$x, y = dat$y, z=dat$Z, B = B, mode = "separate"))
system.time(out2 <- PREGtest(x= dat$x, y = dat$y, z=dat$Z, B = B, mode = "jont"))
alpha = 0.05
mean(out1$res$unsigned<=alpha )
mean(out2$res$unsigned<=alpha )
mean(pval_ttest<=alpha)
system.time(out3 <- FLtest(x= dat$x, y = dat$y, z=dat$Z, B = B))
system.time(out4 <- PERMtest(x= dat$x, y = dat$y, z=dat$Z, B = B))
mean(out3$res$unsigned<=alpha )
mean(out4$res$unsigned<=alpha )
set.seed(2024)
B = 2000
M = 2000
args = list()
###uncorrelated gaussian design, should suffer less from
args$D = "AnovaBalance"; args$E = "multinomial"; args$S=0; args$p=6;args$n=100
dat = data_gen(n=as.integer(args$n), p = as.integer(args$p), M = M, design =args$D, noise = args$E,
seed = seeds[it], m = 1)
if(as.numeric(args$S) <=0.0){
dat$beta = 0.0
}else{
dat$beta  = beta_gen(dat, power = as.numeric(args$S))
}
dat$y = y_gen(dat$x, dat$beta, dat$epsMat)
###t test
fitted_ttest = summary(lm(dat$y~dat$x+dat$Z))
pval_ttest = as.vector(sapply(fitted_ttest, function(z) z$coefficients[2,4]))
mean(pval_ttest<=0.05)
# ###append randomized rows
# L =50
# pval_ttest_noisy = matrix(NA, M,ncol = L)
# for(l in 1:L){
#   fitted_ttest1 = summary(lm(dat$y~dat$Z+dat$Z[sample(1:args$n, args$n, replace=F),]+dat$x))
#   pval_ttest_noisy[,l] = as.vector(sapply(fitted_ttest1, function(z) z$coefficients[nrow(z$coefficients),4]))
# }
# apply(pval_ttest_noisy<=0.05, 2,mean)
###is this because of the randomness in the permutation that mess things up?
# system.time(out1 <- permutation_PREGSseparate(x= dat$x, Y = dat$y, Z=dat$Z, B = B))
# system.time(out3 <- permutation_FL( x = dat$x, Y = dat$y,  Z=dat$Z, B = B))
# system.time(out4 <-permutation_vanilla(x = dat$x, Y = dat$y,  Z=dat$Z, B = B))
#
system.time(out1 <- PREGtest(x= dat$x, y = dat$y, z=dat$Z, B = B, mode = "separate"))
system.time(out2 <- PREGtest(x= dat$x, y = dat$y, z=dat$Z, B = B, mode = "jont"))
alpha = 0.05
mean(out1$res$unsigned<=alpha )
mean(out2$res$unsigned<=alpha )
mean(pval_ttest<=alpha)
system.time(out3 <- FLtest(x= dat$x, y = dat$y, z=dat$Z, B = B))
system.time(out4 <- PERMtest(x= dat$x, y = dat$y, z=dat$Z, B = B))
mean(out3$res$unsigned<=alpha )
mean(out4$res$unsigned<=alpha )
source('~/Dropbox/exactInference/code/PairedRegression/PREGS/R/CPT_helpers.R')
dim(dat$Z)
set.seed(2024)
B = 2000
M = 2000
args = list()
###uncorrelated gaussian design, should suffer less from
args$D = "AnovaBalance"; args$E = "multinomial"; args$S=0; args$p=21;args$n=100
dat = data_gen(n=as.integer(args$n), p = as.integer(args$p), M = M, design =args$D, noise = args$E,
seed = seeds[it], m = 1)
if(as.numeric(args$S) <=0.0){
dat$beta = 0.0
}else{
dat$beta  = beta_gen(dat, power = as.numeric(args$S))
}
dat$y = y_gen(dat$x, dat$beta, dat$epsMat)
###t test
fitted_ttest = summary(lm(dat$y~dat$x+dat$Z))
pval_ttest = as.vector(sapply(fitted_ttest, function(z) z$coefficients[2,4]))
mean(pval_ttest<=0.05)
rounds <- 100 + 900; m = 19;M = NULL
X0 = cbind(dat$x, dat$Z[,-ncol(dat$Z)])
X = cbind(dat$x, dat$Z)
res1 <- find_eta_GA(X0, m, testinds = 1, popSize = 10, rounds = rounds,M = M)
ordering = res1$ordering
ordering
res1$ga_obj
res1$ga_obj$bestFit()
l=1
out0 = CPT(dat$y[,l], X, ordering = ordering,testinds = 1,alpha = 0.05,returnCI = FALSE)
out0
pvals_collection = data.frame(matrix(NA, ncol = 5+1, nrow = ncol(dat$y)))
for(l in 1:ncol(dat$y)){
out0 = CPT(dat$y[,l], X, ordering = ordering,testinds = 1,alpha = 0.05,returnCI = FALSE)
pvals_collection[l,6]=  out0$pval[1]
}
hist(pvals_collection[,6], breaks = 100)
ordering
ordering=1:100
out0 = CPT(dat$y[,l], X, ordering = ordering,testinds = 1,alpha = 0.05,returnCI = FALSE)
out0
out0$eta%*%dat$Z
out0$eta[(1:n)+10]%*%dat$Z
out0$eta[(1:100)+10]%*%dat$Z
out0$eta%*%dat$Z
ww = rep(out0$eta, 100)
out0$eta[(1:100)+10]%*%dat$Z
w[(1:100)+10]%*%dat$Z
ww[(1:100)+10]%*%dat$Z
ww[(1:100)+1]%*%dat$Z
ww[(1:100)+5]%*%dat$Z
ww[(1:100)+15]%*%dat$Z
ww[(1:100)+20]%*%dat$Z
ww[(1:100)+25]%*%dat$Z
dat$Z
B = 2000
M = 2000
args = list()
###uncorrelated gaussian design, should suffer less from
args$D = "Gaussian"; args$E = "multinomial"; args$S=0; args$p=21;args$n=100
dat = data_gen(n=as.integer(args$n), p = as.integer(args$p), M = M, design =args$D, noise = args$E,
seed = seeds[it], m = 1)
if(as.numeric(args$S) <=0.0){
dat$beta = 0.0
}else{
dat$beta  = beta_gen(dat, power = as.numeric(args$S))
}
dat$y = y_gen(dat$x, dat$beta, dat$epsMat)
out0 = CPT(dat$y[,l], X, ordering = ordering,testinds = 1,alpha = 0.05,returnCI = FALSE)
out0
dim(dat$Z)
ww = rep(out0$eta, 100)
ww[(1:100)]%*%dat$Z
ww[(1:100)+5]%*%dat$Z
ww[(1:100)+10]%*%dat$Z
ww[(1:100)+100]%*%dat$Z
ww[(1:100)+10]%*%dat$Z
ww[(1:100)+1]%*%dat$Z
ww[(1:100)+5]%*%dat$Z
ww[(1:100)+10]%*%dat$Z
ww[(1:100)+15]%*%dat$Z
out0
for(l in 1:ncol(dat$y)){
out0 = CPT(dat$y[,l], X, ordering = ordering,testinds = 1,alpha = 0.05,returnCI = FALSE)
pvals_collection[l,6]=  out0$pval[1]
}
m
ww = rep(out0$eta, 100)
ww[(1:100)]%*%dat$Z
ww[(1:100)+5]%*%dat$Z
ww[(1:100)+4]%*%dat$Z
y = dat$y[,l]
X =cbind(dat$x, dat$Z)
ordering
testinds=1
R= NULL
temp <- preprocess_X(X, R, testinds)
all.equal(temp$X, X)
temp$ntestinds
X <- temp$X
ntestinds <- temp$ntestinds
tmp = solve_optim_eta(X, m, ntestinds)
m
tmp
n=nrow(X)
n <- nrow(X)
Piinds <- left_shift(n, m)
Piinds
tmp
tmp$etastar
eta <- tmp$etastar
Piinds <- left_shift(n, m)
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats
eta
y
Piinds
eta
dat$y[,1]
y = dat$y[,1]
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats
stats2 <- apply(Piinds, 2, function(inds){
apply(dat$Z[inds], 2,function(z) sum(z*eta))
})
stats2 <- apply(Piinds, 2, function(inds){
apply(dat$Z[inds,], 2,function(z) sum(z*eta))
})
stats2
stats <- abs(stats - median(stats))
stats
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats
y
y = dat$y[,1]
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats
abs(stats - median(stats))
all.equal(stats[1], stats[2])
stats[,2] - stats[,1]
stats2[,2] - stats2[,1]
eta
eta
unique( eta)
table( eta)
rank(-stats, ties.method = "max") / (m + 1)
tmp = solve_optim_eta(X, m, ntestinds)
eta <- tmp$etastar
Piinds <- left_shift(n, m)
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats <- abs(stats - median(stats))
rank(-stats, ties.method = "max") / (m + 1)
tmp = solve_optim_eta(X, m, ntestinds)
eta <- tmp$etastar
Piinds <- left_shift(n, m)
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats <- abs(stats - median(stats))
rank(-stats, ties.method = "max") / (m + 1)
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats <- abs(stats - median(stats))
rank(-stats, ties.method = "max") / (m + 1)
tmp = solve_optim_eta(X, m, ntestinds)
eta <- tmp$etastar
Piinds <- left_shift(n, m)
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats <- abs(stats - median(stats))
rank(-stats, ties.method = "max") / (m + 1)
tmp = solve_optim_eta(X, m, ntestinds)
eta <- tmp$etastar
Piinds <- left_shift(n, m)
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats <- abs(stats - median(stats))
rank(-stats, ties.method = "max") / (m + 1)
stats2 <- apply(Piinds, 2, function(inds){
apply(dat$Z[inds,], 2,function(z) sum(z*eta))
})
stats2
dim(stats2)
eq_mat = matrix(NA, ncol = m+1, nrow = m+1)
for(l in 1:ncol(stats2)){
for(j in 1:ncol(stats2)){
eq_mat[l,j]=all.equal(stats2[,l],stats2[,j])
}
}
eq_mat
mean(eq_mat)
eq_mat = matrix(NA, ncol = m+1, nrow = m+1)
for(l in 1:ncol(stats2)){
for(j in 1:ncol(stats2)){
eq_mat[l,j]=mean(stats2[,l]==stats2[,j])
}
}
eq_mat
###uncorrelated gaussian design, should suffer less from
args$D = "Gaussian"; args$E = "multinomial"; args$S=0; args$p=2;args$n=100
dat = data_gen(n=as.integer(args$n), p = as.integer(args$p), M = M, design =args$D, noise = args$E,
seed = seeds[it], m = 1)
if(as.numeric(args$S) <=0.0){
dat$beta = 0.0
}else{
dat$beta  = beta_gen(dat, power = as.numeric(args$S))
}
dat$y = y_gen(dat$x, dat$beta, dat$epsMat)
rounds <- 100 + 900; m = 19;M = NULL
X0 = cbind(dat$x, dat$Z[,-ncol(dat$Z)])
X = cbind(dat$x, dat$Z)
tmp = solve_optim_eta(X, m, ntestinds)
eta <- tmp$etastar
Piinds <- left_shift(n, m)
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats
eta <- tmp$etastar
Piinds <- left_shift(n, m)
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats2 <- apply(Piinds, 2, function(inds){
apply(dat$Z[inds,], 2,function(z) sum(z*eta))
})
eq_mat = matrix(NA, ncol = m+1, nrow = m+1)
for(l in 1:ncol(stats2)){
for(j in 1:ncol(stats2)){
eq_mat[l,j]=mean(stats2[,l]==stats2[,j])
}
}
eq_mat
stats2[,1]-stats2[,2]
stats2[,1]
stats2[,2]
?all.equal
eta <- tmp$etastar
Piinds <- left_shift(n, m)
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats2 <- apply(Piinds, 2, function(inds){
apply(dat$Z[inds,], 2,function(z) sum(z*eta))
})
eq_mat = matrix(NA, ncol = m+1, nrow = m+1)
for(l in 1:ncol(stats2)){
for(j in 1:ncol(stats2)){
eq_mat[l,j]=mean(stats2[,l]==stats2[,j], tolerance = .Machine$double.eps)
}
}
eq_mat
eta <- tmp$etastar
Piinds <- left_shift(n, m)
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats2 <- apply(Piinds, 2, function(inds){
apply(dat$Z[inds,], 2,function(z) sum(z*eta))
})
eq_mat = matrix(NA, ncol = m+1, nrow = m+1)
for(l in 1:ncol(stats2)){
for(j in 1:ncol(stats2)){
eq_mat[l,j]=all.equal(stats2[,l]==stats2[,j], tolerance = .Machine$double.eps)
}
}
eta <- tmp$etastar
Piinds <- left_shift(n, m)
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats2 <- apply(Piinds, 2, function(inds){
apply(dat$Z[inds,], 2,function(z) sum(z*eta))
})
eq_mat = matrix(NA, ncol = m+1, nrow = m+1)
for(l in 1:ncol(stats2)){
for(j in 1:ncol(stats2)){
eq_mat[l,j]=all.equal(stats2[,l],stats2[,j], tolerance = .Machine$double.eps)
}
}
eq_mat
set.seed(2024)
B = 2000
M = 2000
args = list()
###uncorrelated gaussian design, should suffer less from
args$D = "Gaussian"; args$E = "multinomial"; args$S=0; args$p=21;args$n=100
dat = data_gen(n=as.integer(args$n), p = as.integer(args$p), M = M, design =args$D, noise = args$E,
seed = seeds[it], m = 1)
if(as.numeric(args$S) <=0.0){
dat$beta = 0.0
}else{
dat$beta  = beta_gen(dat, power = as.numeric(args$S))
}
dat$y = y_gen(dat$x, dat$beta, dat$epsMat)
rounds <- 100 + 900; m = 19;M = NULL
X0 = cbind(dat$x, dat$Z[,-ncol(dat$Z)])
X = cbind(dat$x, dat$Z)
tmp = solve_optim_eta(X, m, ntestinds)
eta <- tmp$etastar
Piinds <- left_shift(n, m)
mean(eq_mat=="TRUE")
eta <- tmp$etastar
Piinds <- left_shift(n, m)
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats2 <- apply(Piinds, 2, function(inds){
apply(dat$Z[inds,], 2,function(z) sum(z*eta))
})
eq_mat = matrix(NA, ncol = m+1, nrow = m+1)
for(l in 1:ncol(stats2)){
for(j in 1:ncol(stats2)){
eq_mat[l,j]=all.equal(stats2[,l],stats2[,j], tolerance = .Machine$double.eps)
}
}
mean(eq_mat=="TRUE")
.Machine
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
stats <- apply(Piinds, 2, function(inds){
sum(y[inds] * eta)
})
rank(stats)
rank(stats, ties.method = "max")
rank(stats, ties.method = "random")
rank(stats, ties.method = "random")
